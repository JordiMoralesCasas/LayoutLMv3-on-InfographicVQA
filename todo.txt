upload to github

docvqa_runner to generalized runner maybe it is not possible

cosine/negative exponential scheduler might be more useful

MORE PARAMETERS FOR RUNNER:
    unify model_folder and checkpoint (maybe checkpoint being a boolean for training frm the model_folder model), use the pretained_model arg
    results file name
    Choose dataset
    Choose scheduler/optimizer (not prioritary rn)
    TODO in modelling.get_optimizers

-- NOT PRIORITY:
Document all the functions

Get loss history (plot graph)

Solve the fast tokenizer call warning

Cache dataset files and images all at once (cache dataset after tokenization/data collation)

Randomly show some images along with questions/gt answer/predicted answer

-- USEFULL:
ANLS metric https://rrc.cvc.uab.es/?ch=11&com=tasks

------- PARA PROXIMA REUNION -------

** Como utiliza el modelo los tokens del contexto (texto del documento/infographic) y la pregunta

** Mirar que utiliza la loss

** Entrenar ultimas capas solo? 
    Segun el paper original, para docVQA hace un finetunning a partir del modelo base:
        Batch size 128 (imposible xd)
        LR 3e-5
        Warmup ratio 0.48 - https://github.com/huggingface/transformers/issues/6673

** Ver porcentage de extractive (concretamente de respuesta explicita en el texto)

** Aproximar resultados de DocVQA a los dados por el paper original
    Con OCR por defecto:
        - Train set: Extraer correctamente 36759 respuestas (Fallan 2704) -> 0.9646 ANLS
        - Val set: Extraer correctamente 4950 respuestas (Fallan 399) -> 0.9611 ANLS
    Con OCR de la API de Miscrosoft:
        - Train set: Extraer correctamente 37888 respuestas (Fallan 1575) -> 0.9819 ANLS
        - Val set: Extraer correctamente 5090 respuestas (Fallan 259) -> 0.9742 ANLS

** Dar una BASELINE para infographicVQA
https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/quickstarts-sdk/client-library?tabs=visual-studio&pivots=programming-language-python

** Mirar y apuntar las dimensiones del docvqa que tengo en cache


accelerate launch \
    --config_file launch_config.yaml docvqa_runner.py \
    --dataset_file cached_datasets/docvqa_cached_extractive_all_lowercase_True_extraction_v1_v2_enumeration \
    --num_epochs 40 \
    --save_model \
    --model_folder docvqa_finetune_base_40epoch
    
accelerate launch \
    --config_file launch_config.yaml docvqa_runner.py \
    --dataset_file cached_datasets/docvqa_cached_extractive_all_lowercase_True_msr_ocr_True_extraction_v1_enumeration \
    --num_epochs 15 \
    --model_folder docvqa_msr_ocr_finetune_base_30epoch \
    --load_state docvqa_msr_ocr_finetune_base_30epoch \
    --save_model

    accelerate launch \
        --config_file launch_config.yaml docvqa_runner.py \
        --dataset_file cached_datasets/docvqa_cached_extractive_all_lowercase_True_msr_ocr_True_extraction_v1_enumeration \
        --mode val \
        --model_folder docvqa_msr_ocr_finetune_base_30epoch \
        --load_state docvqa_msr_ocr_finetune_base_30epoch



-- Forma OCR
para cada pregunta se tienen las mismas palabras

