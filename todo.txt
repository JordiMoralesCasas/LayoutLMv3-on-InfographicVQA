For generation, fix error in tokenizer (fixed?)

How does the decoder implement cross-attention (what information from the encoder uses?)

Better load_state section

change boolean console parameters (from 1/0 to bool)

multiple embedding can use batches bigger than 1 -> change real_batch_size parameters

Train the baseline model with the augmented docvqa datset 

MORE PARAMETERS FOR RUNNER:
    unify model_folder and checkpoint (maybe checkpoint being a boolean for training frm the model_folder model), use the pretained_model arg
    results file name
    Choose scheduler/optimizer (not prioritary rn)
    TODO in modelling.get_optimizers

Document all the functions

When using load_state, do evaluation before training so we don't overwrite the previous best model if the new is worse


